# agents/editor_agent.py
# –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã:
# %pip install llama-index-program-openai
# %pip install llama-index-llms-llama-api
# !pip install llama-index

from llama_index.core.agent import ReActAgent
# from llama_index.llms.openai import OpenAI
from llama_index.core.tools import FunctionTool
# from llama_index.llms.llama_api import LlamaAPI

# ‚úÖ –ü–†–û–°–¢–û–ï –†–ï–®–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π OpenAI –∫–ª–∏–µ–Ω—Ç
from agents.nebius_simple import create_nebius_llm

import re
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def handle_reasoning_failure(callback_manager, exception):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π"""
    if "max iterations" in str(exception).lower():
        return """Based on the analysis completed so far:

The text has been reviewed and appears to meet basic requirements. 
Some minor improvements may be beneficial but are not critical.
The text can proceed to the next stage of processing.

Status: Approved with partial analysis due to iteration limit."""

    return f"Analysis completed with limitations: {str(exception)}"

class EditorAgent:
    def __init__(self, nebius_api_key: str, use_react: bool = False):
        #Args: use_react: –ï—Å–ª–∏ True - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ReAct –∞–≥–µ–Ω—Ç–∞, –µ—Å–ª–∏ False - –ø—Ä—è–º—ã–µ –≤—ã–∑–æ–≤—ã;
        # –ü—Ä—è–º–æ–π Nebius LLM
        self.llm = create_nebius_llm(
            api_key=nebius_api_key,
            model="meta-llama/Llama-3.3-70B-Instruct-fast",
            # model="deepseek-ai/DeepSeek-R1-fast",
            temperature=0.7
        )

        # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞–≥–µ–Ω—Ç–∞ (–±–µ–∑ —è–∑—ã–∫–æ–≤–æ–≥–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞)
        self.tools = [
            self._create_text_validation_tool(),
            self._create_grammar_correction_tool(),   # –ù–ï —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç self.llm
            self._create_semantic_check_tool(),
            self._create_approval_tool()
        ]

        self.use_react = use_react

        # –°–æ–∑–¥–∞–Ω–∏–µ ReAct –∞–≥–µ–Ω—Ç–∞, –µ—Å–ª–∏ –Ω—É–∂–µ–Ω
        if self.use_react:
            self.agent = ReActAgent.from_tools(
                tools=self.tools,
                llm=self.llm,
                verbose=True,
                max_iterations=10,
                handle_reasoning_failure_fn=handle_reasoning_failure,  # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫
                system_prompt=self._get_system_prompt()
            )

    def process_and_improve_text(self, user_text: str) -> dict:
        """–í—ã–±–æ—Ä –º–µ–∂–¥—É ReAct –∞–≥–µ–Ω—Ç–æ–º –∏ –ø—Ä—è–º—ã–º–∏ –≤—ã–∑–æ–≤–∞–º–∏"""
        if self.use_react:
            return self._process_with_react(user_text)  # ‚úÖ –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞
        else:
            return self._process_direct(user_text)
    # self.start_time = None  # –î–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏

    def _process_with_react(self, user_text: str) -> dict:
        """‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ ReAct –∞–≥–µ–Ω—Ç–∞"""
        import time

        start_time = time.time()

        # –†–∞–Ω–Ω—è—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
        words = user_text.split()
        word_count = len(words)

        if word_count < 50:
            processing_time = time.time() - start_time
            return {
                "status": "insufficient_length",
                "original_text": user_text,
                "improved_text": user_text,
                "approved": False,
                "message": f"""**üìù Text Too Short ({word_count} words)**

                        Your text contains only {word_count} words, but our system requires 
                        a minimum of 50 words for proper movie plot analysis.

                        Why 50 words?
                        - Enables accurate semantic analysis
                        - Ensures sufficient plot detail for matching
                        - Improves recommendation quality

                        Please expand your plot description with:*
                        - More character details
                        - Additional plot points
                        - Setting information  
                        - Conflict development

                        Example format:
                        "A young wizard discovers he has magical powers when he receives 
                        a letter to attend Hogwarts School. At school, he learns about his 
                        past and must face the dark wizard who killed his parents. Along with 
                        his friends, he uncovers secrets about the school and fights against 
                        evil forces threatening the wizarding world."

                        Current length: {word_count}/50 words required

                        Please rewrite your plot description with at least 50 words 
                        and try again.""",
                "word_count": word_count,
                "min_required": 50,
                "total_processing_time": round(processing_time, 3),
                "early_termination": True
            }

        # –°—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ª–æ–≥–∏–∫–∞ —Å ReAct –∞–≥–µ–Ω—Ç–æ–º
        prompt = f"""
         Please review and improve this plot description efficiently:
         
         Text: "{user_text}"
         
        Tasks (complete in 5-7 steps maximum):
            1. Validate length (50 or more words) and structure
            2. Correct any grammatical errors and typos  
            3. Check semantic coherence
            4. Approve if requirements are met

        IMPORTANT: Be efficient. Try to complete the task quickly.
        If the text is already acceptable, just approve it.
         """

        try:
            response = self.agent.chat(prompt)
            logger.info(f"response: {response}")
            result = self._parse_editor_response(response, user_text)
            logger.info(f"result: {result}")

        except ValueError as e:
            if "max iterations" in str(e).lower():
                # ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: Fallback –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞
                print(f"Editor reached max iterations, providing fallback result")
                result = {
                    "status": "approved",  # –û–¥–æ–±—Ä—è–µ–º –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
                    "original_text": user_text,
                    "improved_text": user_text,  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
                    "message": "Text analysis completed with basic validation. The text appears acceptable for processing.",
                    "approved": True,
                    "iteration_limit_reached": True,
                    "fallback_used": True
                }
            else:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö ValueError
                raise e

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if start_time:
            total_processing_time = time.time() - start_time
            result["total_processing_time"] = round(total_processing_time, 3)

        return result

    def _process_direct(self, user_text: str) -> dict:
        """–ü—Ä—è–º–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ ReAct –∞–≥–µ–Ω—Ç–∞ - –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ"""
        import time

        start_time = time.time()

        # 1. –†–∞–Ω–Ω—è—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
        words = user_text.split()
        word_count = len(words)

        if word_count < 50:
            processing_time = time.time() - start_time
            return {
                "status": "insufficient_length",
                "original_text": user_text,
                "improved_text": user_text,
                "approved": False,
                "message": f"Text too short: {word_count} words. Minimum required: 50 words.",
                "word_count": word_count,
                "min_required": 50,
                "total_processing_time": round(processing_time, 3),
                "early_termination": True
            }

        # ‚úÖ –ü–†–Ø–ú–´–ï –í–´–ó–û–í–´ –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í

        # 2. –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        validation_tool = self._create_text_validation_tool()
        validation_result = validation_tool.fn(user_text)

        if not validation_result["valid"]:
            processing_time = time.time() - start_time
            return {
                "status": "needs_improvement",
                "original_text": user_text,
                "improved_text": user_text,
                "approved": False,
                "message": f"Validation failed: {', '.join(validation_result['issues'])}",
                "validation_result": validation_result,
                "total_processing_time": round(processing_time, 3)
            }

        # 3. –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è
        grammar_tool = self._create_grammar_correction_tool()
        grammar_result = grammar_tool.fn(user_text)

        # 4. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º corrected_text –µ—Å–ª–∏ –µ—Å—Ç—å)
        text_to_check = grammar_result.get("corrected_text", user_text)
        semantic_tool = self._create_semantic_check_tool()
        semantic_result = semantic_tool.fn(text_to_check)

        # ‚úÖ –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–û–ï –ü–†–ò–ù–Ø–¢–ò–ï –†–ï–®–ï–ù–ò–Ø

        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ–¥–æ–±—Ä–µ–Ω–∏—è
        approval_criteria = {
            "validation_passed": validation_result["valid"],
            "grammar_score": grammar_result.get("improvement_score", 0.0),
            "semantic_coherent": semantic_result.get("coherent", False),
            "corrections_made": grammar_result.get("corrections_made", False)
        }

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ grammar threshold
        grammar_threshold = 0.8
        meets_grammar_threshold = approval_criteria["grammar_score"] >= grammar_threshold

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
        approved = (
                approval_criteria["validation_passed"] and
                approval_criteria["semantic_coherent"] and
                meets_grammar_threshold
        )

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        final_text = grammar_result.get("corrected_text", user_text) if approval_criteria[
            "corrections_made"] else user_text

        processing_time = time.time() - start_time

        if approved:
            return {
                "status": "approved",
                "original_text": user_text,
                "improved_text": final_text,
                "approved": True,
                "message": f"‚úÖ Text approved! Quality score: {approval_criteria['grammar_score']:.2f}/1.0",
                "approval_criteria": approval_criteria,
                "tool_results": {
                    "validation": validation_result,
                    "grammar": grammar_result,
                    "semantics": semantic_result
                },
                "total_processing_time": round(processing_time, 3)
            }
        else:
            # –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –ø—Ä–∏—á–∏–Ω–∞—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
            rejection_reasons = []

            if not meets_grammar_threshold:
                rejection_reasons.append(
                    f"Grammar quality below threshold: {approval_criteria['grammar_score']:.2f} < {grammar_threshold}")

            if not approval_criteria["semantic_coherent"]:
                rejection_reasons.append("Text lacks semantic coherence")

            return {
                "status": "needs_improvement",
                "original_text": user_text,
                "improved_text": final_text,
                "approved": False,
                "message": f"‚ùå Text needs improvement:\n- " + "\n- ".join(rejection_reasons),
                "approval_criteria": approval_criteria,
                "tool_results": {
                    "validation": validation_result,
                    "grammar": grammar_result,
                    "semantics": semantic_result
                },
                "total_processing_time": round(processing_time, 3)
            }

    @staticmethod
    def _get_system_prompt() -> str:
        """–°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        return """You are an Editor Agent for English plot descriptions.

    Your task: Quickly validate and improve text quality (minimum 50 words, proper grammar).

    EFFICIENT Process (3-5 steps maximum):
    1. Use validate_text to check basic requirements
    2. Use correct_grammar to fix issues and get improvement_score  
    3. Use check_semantics to verify plot coherence
    4. Use approve_text ONLY when all criteria are met:
       - improvement_score > 0.8 from grammar correction
       - semantic coherence confirmed
       - validation requirements passed

    The approve_text tool will automatically integrate 
    results from all previous checks."""

    @staticmethod
    def _create_text_validation_tool() -> FunctionTool:
        """–°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""
        def validate_text_requirements(text: str) -> dict:
            """Validate if text meets length and structure requirements"""
            words = text.split()
            word_count = len(words)

            sentences = re.split(r'[.!?]+', text.strip())
            sentences = [s.strip() for s in sentences if s.strip()]

            issues = []

            # Word count check
            if word_count < 50:
                issues.append(f"Text too short: {word_count} words (minimum 50)")
            elif word_count > 500:
                issues.append(f"Text too long: {word_count} words (maximum 500)")

            # Sentence structure check
            if len(sentences) < 2:
                issues.append("Text should contain at least 2 sentences")

            for i, sentence in enumerate(sentences):
                if len(sentence.split()) < 3:
                    issues.append(f"Sentence {i + 1} is too short")

            return {
                "valid": len(issues) == 0,
                "word_count": word_count,
                "sentence_count": len(sentences),
                "issues": issues
            }

        return FunctionTool.from_defaults(
            fn=validate_text_requirements,
            name="validate_text",
            description="Validate if text meets length and structural requirements"
        )

    def _create_grammar_correction_tool(self) -> FunctionTool:
        """–ù–ï —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç self.llm –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–π –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏"""
        def correct_grammar_with_llm(text: str) -> dict:
            """Correct grammatical errors and typos in the text. Real grammar correction using LLM"""
            try:
                correction_prompt = f"""
                Please correct any grammatical errors, typos, and improve the clarity of this text while preserving its meaning:
    
                "{text}"
    
                Requirements:
                - Fix grammatical errors
                - Correct spelling mistakes
                - Improve sentence structure if needed
                - Maintain the original plot and meaning
                - Keep it concise and engaging
    
                Return only the corrected text without explanations.
                """

                # –ò–º–∏—Ç–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã LLM –≤—ã–∑–æ–≤)
                # corrected_text = text  # Placeholder
                # –†–µ–∞–ª—å–Ω—ã–π LLM –≤—ã–∑–æ–≤ —á–µ—Ä–µ–∑ self.llm
                response = self.llm.complete(correction_prompt)
                corrected_text = response.text.strip()

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏
                corrections_made = corrected_text.lower() != text.lower()
                word_diff = abs(len(corrected_text.split()) - len(text.split()))

                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —É–ª—É—á—à–µ–Ω–∏—è
                improvement_score = min(1.0, max(0.5, 1.0 - (word_diff / len(text.split()))))

                return {
                    "corrected_text": corrected_text,
                    # "corrections_made": True,
                    "corrections_made": corrections_made,
                    "improvement_score": 0.85,      # –∑–∞–≥–ª—É—à–∫–∞ –ø—Ä–æ—Ç–∏–≤ —Å–ª–∏—à–∫–æ–º –ø—Ä–∏–¥–∏—Ä—á–∏–≤—ã—Ö llm )
                    # "improvement_score": improvement_score,
                    "original_length": len(text.split()),
                    "corrected_length": len(corrected_text.split())
                }

            except Exception as e:
                # Fallback: –≤–æ–∑–≤—Ä–∞—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                print(f"–û—à–∏–±–∫–∞ LLM –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: {e}")
                return {
                    "corrected_text": text,
                    "corrections_made": False,
                    "improvement_score": 0.0,
                    "error": str(e)
                }

        return FunctionTool.from_defaults(
            fn=correct_grammar_with_llm,
            name="correct_grammar",
            description="Correct grammatical errors and improve text clarity"
        )

    @staticmethod
    def _create_semantic_check_tool() -> FunctionTool:
        """–°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏"""
        def check_semantic_coherence(text: str) -> dict:
            """Check if the text is semantically coherent and well-structured"""
            sentences = re.split(r'[.!?]+', text.strip())
            sentences = [s.strip() for s in sentences if s.strip()]

            issues = []

            # Basic coherence checks
            if len(sentences) < 2:
                issues.append("Need more sentences for proper plot development")

            # Check for plot elements
            plot_keywords = [
                # –ö–æ–Ω—Ñ–ª–∏–∫—Ç/–î—Ä–∞–º–∞
                "war", "betrayal", "revenge", "corruption", "intrigue", "assassination",
                "struggle", "injustice", "dilemma", "survival", "persecution", "resistance",
                "revolution", "espionage", "conspiracy", "situation", "terrorism", "feud",

                # –û—Ç–Ω–æ—à–µ–Ω–∏—è/–≠–º–æ—Ü–∏–∏
                "romance", "love",  "heartbreak", "friendship",
                "family", "sacrifice", "rivalry", "problems", "betrayal", "jealousy",
                "forgiveness", "redemption", "loneliness", "grief", "hope", "obsession", "devotion", "separation",

                # –ü—Ä–∏–∫–ª—é—á–µ–Ω–∏—è/–î–µ–π—Å—Ç–≤–∏–µ
                "quest", "hunt", "mission", "escape", "chase", "heist", "disaster", "disaster",
                "apocalypse", "invasion", "battle", "duel", "superhero", "vigilante", "kidnapping",
                "investigation", "mystery", "conspiracy", "experiment",

                # –õ–∏—á–Ω–æ—Å—Ç–Ω—ã–π —Ä–æ—Å—Ç
                "age", "self-discovery", "crisis", "transformation",
                "fear", "growth",  "journey", "awakening",
                "underdog", "rebirth",

                # –ù–∞—É–∫–∞/–§–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞
                "ai", "time travel", "space exploration",  "dystopia",
                 "cyberpunk", "robot", "mutant", "superpower",
                "contact", "post-apocalypse", "virtual reality",

                # –ú–∏—Å—Ç–∏–∫–∞/–£–∂–∞—Å—ã
                "haunting", "possession", "curse", "force", "witchcraft", "vampire",
                "zombie", "horror", "slasher", "monster", "ghost", "demon",
                "ritual", "paranormal",

                # –û–±—Å—Ç–∞–Ω–æ–≤–∫–∞/–ê—Ç–º–æ—Å—Ñ–µ—Ä–∞
                "small town", "big city", "jungle", "desert", "ocean", "station", "kingdom",
                "era",  "ancient", "civilization",
                "submarine", "island", "laboratory"
            ]
            has_plot_elements = any(keyword in text.lower() for keyword in plot_keywords)

            if not has_plot_elements:
                issues.append("Text should include clear plot elements (characters, setting, conflict)")

            return {
                "coherent": len(issues) == 0,
                "issues": issues,
                "plot_elements_present": has_plot_elements,
                "readability_score": 0.8
            }

        return FunctionTool.from_defaults(
            fn=check_semantic_coherence,
            name="check_semantics",
            description="Check semantic coherence and plot structure"
        )

    @staticmethod
    def _create_approval_tool() -> FunctionTool:
        """–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –æ–¥–æ–±—Ä–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏"""
        def approve_text_with_validation(text: str) -> dict:
            """ –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–¥–æ–±—Ä–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏"""
            import datetime

            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏
            grammar_tool = self._create_grammar_correction_tool()
            grammar_result = grammar_tool.fn(text)

            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
            semantic_tool = self._create_semantic_check_tool()
            semantic_result = semantic_tool.fn(text)

            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            validation_tool = self._create_text_validation_tool()
            validation_result = validation_tool.fn(text)

            # –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è
            approval_criteria = {
                "grammar_score": grammar_result.get("improvement_score", 0.0),
                "semantic_coherent": semantic_result.get("coherent", False),
                "validation_passed": validation_result.get("valid", False),
                "corrections_needed": grammar_result.get("corrections_made", False)
            }

            # ‚úÖ –ö–õ–Æ–ß–ï–í–ê–Ø –õ–û–ì–ò–ö–ê: –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫

            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑–æ–≤—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
            if not approval_criteria["validation_passed"]:
                return {
                    "approved": False,
                    "text": text,
                    "rejection_reason": "Failed basic validation requirements",
                    "validation_issues": validation_result.get("issues", []),
                    "approval_criteria": approval_criteria
                }

            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏
            if not approval_criteria["semantic_coherent"]:
                return {
                    "approved": False,
                    "text": text,
                    "rejection_reason": "Text lacks semantic coherence",
                    "semantic_issues": semantic_result.get("issues", []),
                    "approval_criteria": approval_criteria
                }

            # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ (improvement_score > 0.8)
            grammar_threshold = 0.8
            if approval_criteria["grammar_score"] < grammar_threshold:
                return {
                    "approved": False,
                    "text": text,
                    "rejection_reason": f"Grammar quality below threshold "
                                        f""
                                        f"({approval_criteria['grammar_score']:.2f} < {grammar_threshold})",
                    "suggested_text": grammar_result.get("corrected_text", text),
                    "approval_criteria": approval_criteria
                }

            # ‚úÖ –£–°–ü–ï–®–ù–û–ï –û–î–û–ë–†–ï–ù–ò–ï: –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã
            final_text = grammar_result.get("corrected_text", text) if approval_criteria["corrections_needed"] else text

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—É—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –≤ UTC
            current_time = datetime.datetime.utcnow()
            timestamp_iso = current_time.isoformat() + "Z"

            # –†–∞—Å—á–µ—Ç –∏—Ç–æ–≥–æ–≤–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ score
            final_quality_score = (
                    approval_criteria["grammar_score"] * 0.6 +  # 60% - –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞
                    (1.0 if approval_criteria["semantic_coherent"] else 0.0) * 0.3 +  # 30% - —Å–µ–º–∞–Ω—Ç–∏–∫–∞
                    (1.0 if approval_criteria["validation_passed"] else 0.0) * 0.1  # 10% - –≤–∞–ª–∏–¥–∞—Ü–∏—è
            )

            return {
                "approved": True,
                "text": final_text,
                "original_text": text,
                "timestamp": timestamp_iso,
                "quality_score": round(final_quality_score, 3),
                "approval_criteria": approval_criteria,
                "improvements_applied": approval_criteria["corrections_needed"],
                "approval_metadata": {
                    "grammar_score": approval_criteria["grammar_score"],
                    "semantic_passed": approval_criteria["semantic_coherent"],
                    "validation_passed": approval_criteria["validation_passed"],
                    "final_score": round(final_quality_score, 3),
                    "threshold_met": final_quality_score > 0.8,
                    "utc_time": current_time.strftime("%Y-%m-%d %H:%M:%S UTC")
                }
            }

        return FunctionTool.from_defaults(
            fn=approve_text_with_validation,
            name="approve_text",
            description="Final approval based on grammar correction and semantic validation results"
        )

    # @staticmethod
    def _parse_editor_response(self, response, original_text) -> dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ ReAct –∞–≥–µ–Ω—Ç–∞ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
        import re
        import json

        response_text = str(response)
        logger.info(f"response_text: {response_text}")

        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å fallback –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        result = {
            "status": "needs_improvement",
            "original_text": original_text,
            "improved_text": original_text,
            "message": "Processing completed",
            "approved": False,
            "improvement_score": 0.0,
            "quality_metrics": {},
            "tool_results": {}
        }

        # ‚úÖ –ü–ê–†–°–ò–ù–ì –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í

        # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ approve_text (—Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ)
        approve_pattern = r'approve_text.*?(\{[^}]*"approved"[^}]*\})'
        approve_match = re.search(approve_pattern, response_text, re.DOTALL | re.IGNORECASE)

        if approve_match:
            try:
                approve_result = json.loads(approve_match.group(1))
                result["approved"] = approve_result.get("approved", False)
                result["status"] = "approved" if approve_result.get("approved", False) else "needs_improvement"
                result["quality_metrics"]["final_score"] = approve_result.get("quality_score", 0.0)
                result["tool_results"]["approval"] = approve_result

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º improved text –∏–∑ approve_text –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                if "text" in approve_result and approve_result["text"] != original_text:
                    result["improved_text"] = approve_result["text"]

            except json.JSONDecodeError:
                logger.warning("Failed to parse approve_text result")

        # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ correct_grammar (improvement_score –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è)
        grammar_pattern = r'correct_grammar.*?(\{[^}]*"improvement_score"[^}]*\})'
        grammar_match = re.search(grammar_pattern, response_text, re.DOTALL | re.IGNORECASE)

        if grammar_match:
            try:
                grammar_result = json.loads(grammar_match.group(1))
                result["improvement_score"] = grammar_result.get("improvement_score", 0.0)
                result["tool_results"]["grammar"] = grammar_result

                # ‚úÖ –ö–õ–Æ–ß–ï–í–ê–Ø –ü–†–û–í–ï–†–ö–ê: improvement_score > 0.8
                if grammar_result.get("improvement_score", 0.0) > 0.8:
                    result["quality_metrics"]["grammar_threshold_met"] = True

                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º corrected_text –µ—Å–ª–∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –±—ã–ª–∞ —Å–¥–µ–ª–∞–Ω–∞
                    if grammar_result.get("corrections_made", False):
                        corrected_text = grammar_result.get("corrected_text", original_text)
                        if corrected_text != original_text:
                            result["improved_text"] = corrected_text
                else:
                    result["quality_metrics"]["grammar_threshold_met"] = False
                    result["approved"] = False  # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –µ—Å–ª–∏ grammar score –Ω–∏–∑–∫–∏–π
                    result["status"] = "needs_improvement"

            except json.JSONDecodeError:
                logger.warning("Failed to parse correct_grammar result")

        # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ validate_text
        validate_pattern = r'validate_text.*?(\{[^}]*"valid"[^}]*\})'
        validate_match = re.search(validate_pattern, response_text, re.DOTALL | re.IGNORECASE)

        if validate_match:
            try:
                validate_result = json.loads(validate_match.group(1))
                result["tool_results"]["validation"] = validate_result
                result["quality_metrics"]["validation_passed"] = validate_result.get("valid", False)

                if not validate_result.get("valid", False):
                    result["approved"] = False
                    result["status"] = "needs_improvement"

            except json.JSONDecodeError:
                logger.warning("Failed to parse validate_text result")

        # 4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ check_semantics
        semantic_pattern = r'check_semantics.*?(\{[^}]*"coherent"[^}]*\})'
        semantic_match = re.search(semantic_pattern, response_text, re.DOTALL | re.IGNORECASE)

        if semantic_match:
            try:
                semantic_result = json.loads(semantic_match.group(1))
                result["tool_results"]["semantics"] = semantic_result
                result["quality_metrics"]["semantic_coherent"] = semantic_result.get("coherent", False)

                if not semantic_result.get("coherent", False):
                    result["approved"] = False
                    result["status"] = "needs_improvement"

            except json.JSONDecodeError:
                logger.warning("Failed to parse check_semantics result")

        # ‚úÖ –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –î–ï–¢–ê–õ–¨–ù–û–ì–û –°–û–û–ë–©–ï–ù–ò–Ø –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        message_parts = []

        if result["approved"]:
            message_parts.append("‚úÖ **Text approved for processing**")
            if result["improvement_score"] > 0.8:
                message_parts.append(f"üìä Quality score: {result['improvement_score']:.2f}/1.0")
            if result["improved_text"] != original_text:
                message_parts.append("üìù Text has been improved during processing")
        else:
            message_parts.append("‚ùå **Text requires improvement**")

            # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
            if not result["quality_metrics"].get("grammar_threshold_met", True):
                score = result.get("improvement_score", 0.0)
                message_parts.append(f"üìù Grammar quality below threshold: {score:.2f} < 0.8")

            if not result["quality_metrics"].get("validation_passed", True):
                validation_issues = result["tool_results"].get("validation", {}).get("issues", [])
                if validation_issues:
                    message_parts.append(f"üìã Validation issues: {', '.join(validation_issues[:2])}")

            if not result["quality_metrics"].get("semantic_coherent", True):
                semantic_issues = result["tool_results"].get("semantics", {}).get("issues", [])
                if semantic_issues:
                    message_parts.append(f"üß† Semantic issues: {', '.join(semantic_issues[:2])}")

        result["message"] = "\n".join(message_parts) if message_parts else response_text

        # ‚úÖ FALLBACK: –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –ª–æ–≥–∏–∫—É
        if not any([approve_match, grammar_match, validate_match, semantic_match]):
            logger.warning("No tool results found, falling back to keyword search")
            approved = "approved" in response_text.lower() and "true" in response_text.lower()
            result["approved"] = approved
            result["status"] = "approved" if approved else "needs_improvement"
            result["message"] = response_text

        logger.info(f"Parsed result: approved={result['approved']}, improvement_score={result['improvement_score']}")

        return result
